# Flux.1 on Vast.ai

A complete HTTP API server for Flux.1 image generation, optimized for GPU instances on Vast.ai with UUID-based image URLs.

## Features

- ðŸš€ Fast inference with FLUX.1-schnell (4 steps, ~40 seconds)
- ðŸ”— UUID-based image URLs for easy sharing
- ðŸ“¡ RESTful HTTP API with async job queue
- ðŸ’¾ Optimized for 8GB VRAM (RTX 3070/3080/etc)
- ðŸ”„ Sequential CPU offloading for memory efficiency
- âœ… Health check and status endpoints
- ðŸŒ Public internet access via Vast.ai port mapping

## Quick Start

See [QUICKSTART.md](QUICKSTART.md) for one-command setup.

### On Vast.ai (Recommended)

1. Launch a GPU instance with:
   - **GPU:** 8GB+ VRAM (RTX 3070, 3080, etc.)
   - **Image:** PyTorch CUDA template or Ubuntu with CUDA
   - **Disk:** 30GB+ (for models)

2. Clone and setup:
```bash
git clone https://github.com/brandynette/flux1-vast-ai.git
cd flux1-vast-ai
pip install -r requirements.txt
```

3. Set your HuggingFace token:
```bash
export HF_TOKEN='your_hf_token_here'
```

4. Start the server:
```bash
./start.sh
# Or manually:
nohup python3 server.py > server.log 2>&1 &
```

5. Get your public URL:
```bash
echo "http://$PUBLIC_IPADDR:$VAST_TCP_PORT_6006"
```

### Full Setup Guide

See [VAST_AI_SETUP.md](VAST_AI_SETUP.md) for complete deployment instructions.

## Documentation

- **[QUICKSTART.md](QUICKSTART.md)** - One-command setup guide
- **[VAST_AI_SETUP.md](VAST_AI_SETUP.md)** - Complete Vast.ai deployment guide  
- **[SERVER_README.md](SERVER_README.md)** - API documentation
- **[ENVIRONMENT.md](ENVIRONMENT.md)** - Configuration reference

## File Structure

```
flux1-vast-ai/
â”œâ”€â”€ server.py              # HTTP API server (runs on port 6006)
â”œâ”€â”€ flux_inference.py      # CLI inference tool
â”œâ”€â”€ start.sh              # Auto-start script
â”œâ”€â”€ requirements.txt      # Python dependencies
â”œâ”€â”€ README.md            # This file
â”œâ”€â”€ QUICKSTART.md        # Quick setup guide
â”œâ”€â”€ VAST_AI_SETUP.md    # Full deployment guide
â”œâ”€â”€ SERVER_README.md     # API documentation
â””â”€â”€ ENVIRONMENT.md       # Configuration reference
```

### Local Development

```bash
# Install dependencies
pip install -r requirements.txt

# Authenticate with HuggingFace
export HF_TOKEN='your_token'

# Start server
python3 server.py
```

## API Usage

### Generate Image

```bash
curl -X POST http://localhost:5000/generate \
  -H "Content-Type: application/json" \
  -d '{"prompt": "a cat wearing sunglasses", "steps": 4}'
```

**Response:**
```json
{
  "job_id": "550e8400-e29b-41d4-a716-446655440000",
  "status_url": "/status/550e8400-e29b-41d4-a716-446655440000",
  "image_url": "/image/550e8400-e29b-41d4-a716-446655440000"
}
```

### Check Status

```bash
curl http://localhost:5000/status/<job_id>
```

### Download Image

```bash
curl http://localhost:5000/image/<job_id> -o output.png
```

## CLI Tool

```bash
python3 flux_inference.py --prompt "a futuristic city" --output city.png
```

## Performance

- **Model:** FLUX.1-schnell
- **GPU:** RTX 3070 (8GB VRAM)
- **Generation time:** ~40-45 seconds (4 steps, 1024x1024)
- **First run:** ~5 minutes (model download ~17GB)
- **Memory:** Optimized for 8GB VRAM with CPU offloading

## Tested Configuration

- **GPU:** NVIDIA GeForce RTX 3070 (8GB)
- **CUDA:** 12.1.1
- **Driver:** 535.230.02
- **Platform:** Vast.ai
- **Public IP:** Accessible worldwide

See [SERVER_README.md](SERVER_README.md) for full API documentation.
